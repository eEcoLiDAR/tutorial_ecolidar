{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lidar pointcloud analysis using laserfarm\n",
    "\n",
    "In this tutorial, we demonstrate an example of analysing AHN3 pointcloud data using the `laserfarm` package.\n",
    "\n",
    "Laserfarm (Laserchicken Framework for Applications in Research in Macro-ecology) provides a FOSS wrapper to Laserchicken supporting the use of massive LiDAR point cloud data sets for macro-ecology, from data preparation to scheduling and execution of distributed processing across a cluster of compute nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "AHN (Actueel Hoogtebestand Nederland) is a digtal height dataset of the Netherlands, measured with laser altimetry. The [AHN3](https://www.pdok.nl/introductie/-/article/actueel-hoogtebestand-nederland-ahn3-) dataset, which is used in this tutorial, is the third update of this dataset. \n",
    "\n",
    "AHN3 is a public dataset and is free from copyright restrictions. Please refer to [this link](https://data.overheid.nl/en/dataset/11513-actueel-hoogtebestand-nederland-3--ahn3-) for the license status of AHN3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "                    \n",
    "from dask.distributed import LocalCluster\n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a Dask cluster\n",
    "\n",
    "In this tutorial, we will use a Dask cluster for all the macro-pipeline calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_tmp = pathlib.Path('./tmp')\n",
    "cluster = LocalCluster(processes=True, \n",
    "                       n_workers=2, \n",
    "                       threads_per_worker=1, \n",
    "                       local_directory=local_tmp/'dask-worker-space')\n",
    "\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Retiling\n",
    "The first step in the pipeline is to retile the point-cloud files to a regular grid, splitting the original data into smaller chuncks that are easier to handle for data processing.\n",
    "\n",
    "First we will set the relavant path of this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the input AHN3 data\n",
    "path_ahn = pathlib.Path('../data/')\n",
    "\n",
    "# Set the output path\n",
    "path_retiled = pathlib.Path('./retiling/retiling_out/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will set the boundaries of the grid and the number of tiles along each axis. Here we give RD coordinates of the data of interest. \n",
    "\n",
    "Here we set the `n_tiles_side` to 2, then 2x2=4 tiles will be created. Note that if the grid boundaries are within the actual dataset, extra tiles outside the boundary will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the retiling grid\n",
    "grid = {\n",
    "    'min_x': 88500,\n",
    "    'max_x': 88800,\n",
    "    'min_y': 458400,\n",
    "    'max_y': 458700,\n",
    "    'n_tiles_side': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up a dictionary defining the configuration of the retiling step. There are four substeps here:\n",
    "\n",
    "- setup_local_fs: specify the local file paths\n",
    "- set_grid: set the retiling grid\n",
    "- split_and_redistribute: split the pointcloud file in the data directory with given grid\n",
    "- validate: Validate the produced output by checking consistency in the number\n",
    "        of input and output points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup input dictionary to configure the retiling pipeline\n",
    "retiling_input = {\n",
    "    'setup_local_fs': {'tmp_folder': local_tmp.as_posix(),\n",
    "                       'input_folder': path_ahn.as_posix(),\n",
    "                       'output_folder': path_retiled.as_posix()},\n",
    "    'set_grid': grid,\n",
    "    'split_and_redistribute': {},\n",
    "    'validate': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will setup the pipeline for the retiling step. Essentialy, for every input data file in `path_ahn`,  we will initiate a `Retiler` object and configure with `retiling_input`. The `Retiler` objects forms the tasks of the pipeline. The processing then can be distributed to the given cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiate the pipline\n",
    "retiling_macro = MacroPipeline()\n",
    "\n",
    "# Get the pointcloud data files\n",
    "data_files =  [f for f in path_ahn.iterdir()]\n",
    "print('Retrieve and retile: {} LAZ files'.format(len(data_files)))\n",
    "\n",
    "# Initialize Retiler per input data file, and configure. \n",
    "# Add Retiler list to macro-pipeline tasks\n",
    "retiling_macro.tasks = [Retiler(file).config(retiling_input) for file in data_files]\n",
    "\n",
    "# Set the corresponding labels\n",
    "retiling_macro.set_labels([os.path.splitext(file)[0] for file in data_files])\n",
    "\n",
    "# Set the cluster for distributing processing\n",
    "retiling_macro.setup_cluster(cluster=cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can execute the pipeline, and print the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run! \n",
    "retiling_macro.run()\n",
    "retiling_macro.print_outcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can check the output directory and see what files are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in path_retiled.rglob('*') if f.is_file()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction\n",
    "\n",
    "By retiling the original files to smaller tiles, we prepared the processing easier for distribution. Now, for a better understading of the poincloud, we need to investigate the common characters of multiple points within a certain range. \n",
    "\n",
    "To do this, we devide the poincloud into volumes. Within each volume, various statistics are computed to represent the volume. These statistics are referred as \"features\". In each volume, the derived features are assigned to an artificial point representing the volume, which is referred as the \"target point\".  \n",
    "\n",
    "The build-in features can be extracted from `laserchicken`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laserchicken.feature_extractor.feature_extraction import list_feature_names\n",
    "sorted(list_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will compute fourt features within 10m meshgrid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target mesh size & list of features\n",
    "tile_mesh_size = 10.\n",
    "features = ['perc_95_normalized_height', \n",
    "            'pulse_penetration_ratio', \n",
    "            'entropy_normalized_height', \n",
    "            'point_density']\n",
    "\n",
    "# Ouput path of the target points\n",
    "path_targets = pathlib.Path('./feature_extraction/targets/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of feature extraction\n",
    "\n",
    "As the previous step, we set up a dictionary configuring of the feature extraction step. There are seven substeps here:\n",
    "\n",
    "- setup_local_fs: specify the local file paths\n",
    "- load: load retiled data\n",
    "- normalize: Normalize point cloud heights.\n",
    "- generate_targets: generates target points\n",
    "- extract_features: compute the features \n",
    "- export_targets: export target points into files\n",
    "- clear_cache: cache cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_input = {\n",
    "    'setup_local_fs': {\n",
    "        'input_folder': path_retiled.as_posix(),\n",
    "        'output_folder': path_targets.as_posix()\n",
    "    },\n",
    "    'load': {},\n",
    "    'normalize': {\n",
    "        'cell_size': 1\n",
    "    },\n",
    "    'generate_targets': {\n",
    "        'tile_mesh_size' : 10.0,\n",
    "        'validate' : True,\n",
    "        'validate_precision': 1.e-3,\n",
    "        **grid\n",
    "    },\n",
    "    'extract_features': {\n",
    "        'feature_names': features,\n",
    "        'volume_type': 'cell',\n",
    "        'volume_size': 10\n",
    "    },\n",
    "    'export_targets': {\n",
    "        'attributes': features,\n",
    "        'multi_band_files': False\n",
    "    },\n",
    "    'clear_cache': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize a feature extraction pipeline and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "fe_macro = MacroPipeline()\n",
    "\n",
    "# Determine which tiles and extract tile index numbers\n",
    "tiles = [t.name for t in path_retiled.glob('tile_*_*/')]\n",
    "tile_indices = [[int(n) for n in t.split('_')[1:]] for t in tiles]\n",
    "print('Retrieve and process: {} tiles'.format(len(tile_indices)))\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "fe_macro.tasks = [DataProcessing(t, tile_index=idx).config(feature_extraction_input)\n",
    "               for t, idx in zip(tiles, tile_indices)]\n",
    "fe_macro.set_labels(tiles)\n",
    "\n",
    "fe_macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "fe_macro.run()\n",
    "fe_macro.print_outcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also see the results of the four features are generated under `path_targets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in path_targets.rglob('*') if f.is_dir()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: GeoTIFF Export\n",
    "\n",
    "At last, we can export the extracted features from the target points to GeoTIFF files.\n",
    "\n",
    "First we need to setup the paths and input dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dCache path where to copy the GeoTIFF files\n",
    "gw_out_path = pathlib.Path('./geotiffs')\n",
    "\n",
    "# setup input dictionary to configure the GeoTIFF export pipeline\n",
    "geotiff_export_input = {\n",
    "    'setup_local_fs': {'input_folder': path_targets.as_posix(),\n",
    "                       'output_folder': gw_out_path.as_posix()},\n",
    "    'parse_point_cloud': {},\n",
    "    'data_split': [1, 1],\n",
    "    'create_subregion_geotiffs': {'output_handle': 'geotiff'}\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('geotiff_export.json', 'w') as f:\n",
    "    json.dump(geotiff_export_input, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can execute the pipeline to export the geotiffs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_geotiff = MacroPipeline()\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "macro_geotiff.tasks = [GeotiffWriter(input_dir=feature, bands=feature).config(geotiff_export_input)\n",
    "               for feature in features]\n",
    "macro_geotiff.set_labels(features)\n",
    "\n",
    "macro_geotiff.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "macro_geotiff.run()\n",
    "macro_geotiff.print_outcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate cluster\n",
    "\n",
    "At the enc of this tutorial, the cluster can be closed by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
